{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SigOpt for ML: Automatically Tuning Text Classifiers Walkthrough\n",
    "*This <a href=\"http://jupyter.readthedocs.org/en/latest/install.html\">Jupyter notebook</a> is a walkthrough companion for our blog post [SigOpt for ML: Automatically Tuning Text Classifiers](http://blog.sigopt.com/post/133089144983/sigopt-for-ml-automatically-tuning-text) by Research Engineer [Ian Dewancker](https://sigopt.com/about#ian)*\n",
    "\n",
    "Text classification problems appear quite often in modern information systems, and you might imagine building a small document/tweet/blogpost classifier for any number of purposes. In this example, we're building a pipeline to label [Amazon product reviews](http://www.cs.jhu.edu/~mdredze/datasets/sentiment/) as either favorable or not. First, we'll use scikit-learn to build a logistic regression classifier, performing k-fold cross validation to measure the accuracy. Next, we'll use SigOpt to tune the hyperparameters of both the feature extraction and the model building. \n",
    "## Setup\n",
    "If you'd like to try running this example, you'll need to install:\n",
    " * [scikit-learn](http://scikit-learn.org/stable/install.html)\n",
    " * SigOpt's [python client](https://sigopt.com/docs/overview/python)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Learn more about authenticating the SigOpt API: sigopt.com/docs/overview/authentication\n",
    "# Uncomment the following line and add your SigOpt API token:\n",
    "# SIGOPT_API_TOKEN=<YOUR API TOKEN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POSITIVE: My husband is a BMW mechanic and he drives cars all day.  I wish I could get him one for every car\n",
      "\n",
      "NEGATIVE: Far better models for similair price range - dissapointed with the quality of the finish etc etc. works fine though.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json, urllib\n",
    "(negative_file, _ ) = urllib.urlretrieve(\n",
    "    \"http://sigopt-public.s3-website-us-west-2.amazonaws.com/NEGATIVE_list.json\",\n",
    "    \"NEGATIVE_LIST.json\"\n",
    ")\n",
    "(positive_file, _) = urllib.urlretrieve(\n",
    "    \"http://sigopt-public.s3-website-us-west-2.amazonaws.com/POSITIVE_list.json\",\n",
    "    \"POSITIVE_LIST.json\"\n",
    ")\n",
    "POSITIVE_TEXT = json.load(open(positive_file))\n",
    "NEGATIVE_TEXT = json.load(open(negative_file))\n",
    "print \"POSITIVE: {}\".format(POSITIVE_TEXT[0])\n",
    "print \"NEGATIVE: {}\".format(NEGATIVE_TEXT[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Setting up the Text Classifier\n",
    "## Feature Representation\n",
    "The [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) class in scikit-learn is a convenient mechanism for transforming a corpus of text documents into vectors using [bag of words representations (BOW)](https://en.wikipedia.org/wiki/Bag-of-words_model). scikit-learn offers quite a bit of control in determining which [n-grams](https://en.wikipedia.org/wiki/N-gram) make up the vocabulary for your BOW vectors.  As a quick refresher, n-grams are sequences of text tokens as shown in the example below:\n",
    "\n",
    "Original Text | “SigOpt optimizes any complicated system”\n",
    "--- | ---\n",
    "1-grams | { “SigOpt”, “optimizes”, “any”, “complicated”, “system”}\n",
    "2-grams | { “SigOpt_optimizes”, “optimizes_any”, “any_complicated” ... }\n",
    "3-grams | { “SigOpt_optimizes_any”, “optimizes_any_complicated” ... }\n",
    "\n",
    "The number of times each n-gram appears in a given piece of text is then encoded in the BOW vector describing that text.  CountVectorizer allows you to control the range of n-grams that are included in the vocabulary (`ngram_range`), as well as filtering n-grams outside a specified [document-frequency](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) range (`min_df`, `max_df`). For example, if a rare 3-gram like “hi_diddly_ho” doesn’t appear with at least `min_df` frequency in the corpus, it is not included in the vocabulary.  Similarly, n-grams that occur in nearly every document (1-grams like “the”, “a” etc) can also be filtered using the `max_df` parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Create a CountVectorizer with the default parameters\n",
    "vectorizer = CountVectorizer()\n",
    "# Transform corpus into vectors using bag-of-words\n",
    "text_features = vectorizer.fit_transform(POSITIVE_TEXT + NEGATIVE_TEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Error Cost Parameters\n",
    "Using the [SGDClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) class in scikit-learn, we can succinctly formulate and solve the logistic regression learning problem.  For a full description of the error function for logistic regression, see the [original blog post](http://blog.sigopt.com/post/133089144983/sigopt-for-ml-automatically-tuning-text). For brevity, we'll examine the exposed parameters of `alpha`, the weight of the [regularization](https://en.wikipedia.org/wiki/Regularization_(mathematics)) term in our cost function, and `l1_ratio`, which controls the [mixture](https://en.wikipedia.org/wiki/Elastic_net_regularization) of l1 and l2 regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "# Create an SGDClassifier with default parameters\n",
    "classifier = SGDClassifier(loss='log', penalty='elasticnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective Metric\n",
    "SigOpt finds parameter configurations that **maximize** any metric, so we need to pick one that is appropriate for this classification task. In designing our objective metric, [accuracy](https://en.wikipedia.org/wiki/Accuracy_and_precision), the number of correctly classified reviews, is obviously important, but we also want assurance that our model generalizes and can perform well on data on which it was not trained.  This is where the idea of [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) comes into play.  \n",
    "\n",
    "Cross-validation requires us to split up our entire labeled dataset into two distinct sets: one to train on and one to validate our trained classifier on.  We then consider metrics like accuracy on only the validation set.  Taking this further and considering not one, but many possible splits of the labeled data is the idea of [k-fold cross-validation](https://en.wikipedia.org/wiki/Cross-validation_%28statistics%29#k-fold_cross-validation) where multiple training, validation sets are generated and validation metrics can be aggregated in several ways (e.g., mean, min, max) to give a single estimation of performance.\n",
    "\n",
    "In this case, we’ll use the mean of the k-folded cross-validation accuracies (see the [original blog post](http://blog.sigopt.com/post/133089144983/sigopt-for-ml-automatically-tuning-text) for a further discussion). In our case, k=5 folds are used and the train and validation sets are split randomly using 70% and 30% of the entire dataset, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of 5-folded cross-validation accuracies: 0.839165419162\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "cv_split = cross_validation.ShuffleSplit(\n",
    "    text_features.shape[0], \n",
    "    n_iter=5, \n",
    "    test_size=0.3, \n",
    "    random_state=0\n",
    ")\n",
    "# Target classification of 1 for positive sentiment, -1 for negative\n",
    "target = [1] * len(POSITIVE_TEXT) + [-1] * len(NEGATIVE_TEXT)\n",
    "cv_scores = cross_validation.cross_val_score(\n",
    "    classifier, \n",
    "    text_features, \n",
    "    target, \n",
    "    cv=cv_split\n",
    ")\n",
    "import numpy\n",
    "print \"Mean of 5-folded cross-validation accuracies: {}\".format(\n",
    "    numpy.mean(cv_scores)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part II: Tuning the Text Classifier\n",
    "## Tunable Parameters\n",
    "The objective metric is controlled by a set of parameters that potentially influence its performance. While the logistic regression model might be conceptually simple and implemented in many statistics and machine learning software packages, valuable engineering time and resources are often wasted experimenting with feature representation and parameter tuning via trial and error.  SigOpt can automatically and intelligently optimize your objective, letting you get back to working on other tasks.\n",
    "\n",
    "Within SigOpt, [Parameters](https://sigopt.com/docs/objects/parameter) can be defined on integer, continuous, or categorical domains. The first step is to define a SigOpt [Experiment](https://sigopt.com/docs/objects/experiment) with parameters to be tuned.\n",
    "\n",
    "The parameters used in this experiment can be split into two groups: those governing the feature representation of the review text in the `vectorizer` (`min_df`, `max_df` and `ngram_range`) and those governing the cost function of logistic regression in the `classifier` (`alpha`, `l1_ratio`). We can examine the default values of these parameters below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default parameters of vectorizer: {\"ngram_range\": [1, 1], \"max_df\": 1.0, \"min_df\": 1}\n",
      "Default parameters of classifier: {\"alpha\": 0.0001, \"l1_ratio\": 0.15}\n"
     ]
    }
   ],
   "source": [
    "print \"Default parameters of vectorizer: {}\".format(json.dumps({\n",
    "    'min_df': vectorizer.min_df,\n",
    "    'max_df': vectorizer.max_df,\n",
    "    'ngram_range': vectorizer.ngram_range,\n",
    "}))\n",
    "print \"Default parameters of classifier: {}\".format(json.dumps({\n",
    "    'alpha': classifier.alpha,\n",
    "    'l1_ratio': classifier.l1_ratio,\n",
    "}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tricks to Tuning with SigOpt\n",
    "As you can see, `ngram_range` is really two integer parameters, a minimum and a maximum. To tune these parameters in the classifier we tell SigOpt to tune the minimum value as an integer parameter such as `min_ngram`, in addition to an offset such as `ngram_offset`, where `max_ngram = min_ngram + ngram_offset`. We'll use this trick with parameters for both document frequency and n-grams.\n",
    "\n",
    "Often when the range of the parameter is very large or very small, it makes sense to look at the parameter on the [log scale](https://en.wikipedia.org/wiki/Logarithmic_scale), as we'll do with the `log_min_df` and the `log_reg_coef` parameters. To transform back to the original parameter, `min_df = math.exp(log_min_df)`.\n",
    "\n",
    "To finish, we choose reasonable bounds for every parameter, and create a SigOpt experiment with the [python client](https://github.com/sigopt/sigopt-python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Learn more about authenticating the SigOpt API: sigopt.com/docs/overview/authentication\n",
    "from sigopt import Connection\n",
    "conn = Connection(client_token=SIGOPT_API_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "experiment = conn.experiments().create(\n",
    "    name='Sentiment LR Classifier',\n",
    "    project='sigopt-examples',\n",
    "    metrics=[dict(name='cv_scores', objective='maximize')],\n",
    "    parameters=[{ \n",
    "        'name':'l1_coef', \n",
    "        'type': 'double', \n",
    "        'bounds': { 'min': 0, 'max': 1.0 }\n",
    "    }, { \n",
    "        'name':'log_reg_coef', \n",
    "        'type': 'double', \n",
    "        'bounds': { 'min': math.log(0.000001), 'max': math.log(100.0) }\n",
    "    }, { \n",
    "        'name':'min_ngram', \n",
    "        'type': 'int',\n",
    "        'bounds': { 'min': 1, 'max': 2 }\n",
    "    }, { \n",
    "        'name':'ngram_offset',\n",
    "        'type': 'int',\n",
    "        'bounds': { 'min': 0, 'max': 2 }\n",
    "    }, { \n",
    "        'name':'log_min_df', \n",
    "        'type': 'double',\n",
    "        'bounds': { 'min': math.log(0.00000001), 'max': math.log(0.1) }\n",
    "    }, { \n",
    "        'name':'df_offset', \n",
    "        'type': 'double',\n",
    "        'bounds': { 'min': 0.01, 'max': 0.25 }\n",
    "    }],\n",
    ")\n",
    "print \"View your experiment details at https://sigopt.com/experiment/{0}\".format(experiment.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all Together\n",
    "Now that we've defined our experiment for SigOpt, we should re-write our classifier code so that it accepts a dictionary of assignments as they will be returned from the [SigOpt API](https://sigopt.com/docs/overview). Here, our function also accepts the lists of positive and negative sentiment text. You can see the relationship between the parameters defined in the SigOpt experiment and the parameters used by CountVectorizer and SGDClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sentiment_metric(positive, negative, assignments):\n",
    "    min_ngram = assignments['min_ngram']\n",
    "    max_ngram = min_ngram + assignments['ngram_offset']\n",
    "    min_doc_frequency = math.exp(assignments['log_min_df'])\n",
    "    max_doc_frequency = min_doc_frequency + assignments['df_offset']\n",
    "    vectorizer = CountVectorizer(\n",
    "        min_df=min_doc_frequency, \n",
    "        max_df=max_doc_frequency,                          \n",
    "        ngram_range=(min_ngram, max_ngram),\n",
    "    )\n",
    "    text_features = vectorizer.fit_transform(positive + negative)\n",
    "    target = [1] * len(positive) + [-1] * len(negative)\n",
    "    \n",
    "    alpha = math.exp(assignments['log_reg_coef'])\n",
    "    l1_ratio = assignments['l1_coef']\n",
    "    classifier = SGDClassifier(\n",
    "        loss='log', \n",
    "        penalty='elasticnet', \n",
    "        alpha=alpha, \n",
    "        l1_ratio=l1_ratio\n",
    "    )\n",
    "    cv = cross_validation.ShuffleSplit(\n",
    "        text_features.shape[0], \n",
    "        n_iter=5, \n",
    "        test_size=0.3, \n",
    "        random_state=0\n",
    "    )\n",
    "    cv_scores = cross_validation.cross_val_score(classifier, text_features, target, cv=cv)\n",
    "    return numpy.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Loop\n",
    "After creating an experiment, run the [optimization loop](https://sigopt.com/docs/overview/optimization) to tune the parameters:\n",
    "1. Receive suggested assignments from SigOpt\n",
    "2. Calculate the sentiment metric given these assignments\n",
    "3. Report the observed sentiment metric to SigOpt\n",
    "\n",
    "Based on our observations, we advise running iterations equal to 10-20 times the number of parameters. Since we have 6 parameters, we'll run 60 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(60):\n",
    "    print \"Training model candidate {0}\".format(i)\n",
    "    suggestion = conn.experiments(experiment.id).suggestions().create()\n",
    "    opt_metric = sentiment_metric(POSITIVE_TEXT, NEGATIVE_TEXT, suggestion.assignments)\n",
    "    conn.experiments(experiment.id).observations().create(\n",
    "      suggestion=suggestion.id,\n",
    "      value=opt_metric,\n",
    "    ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closing Remarks\n",
    "See the [original blog post](http://blog.sigopt.com/post/133089144983/sigopt-for-ml-automatically-tuning-text) for a discussion of results average over 20 runs of this optimization loop for grid search, random search, and SigOpt, all versus the baseline of no parameter tuning. (Hint: you see accuracy gains with parameter tuning.)\n",
    "\n",
    "This short example scratches the surface of the types of ML related experiments one could conduct using SigOpt.  For example, SGDClassifier has lots of variations from which to select– another experiment might be to treat the loss function as a [categorical variable](https://sigopt.com/docs/objects/parameter).  What sort of models are you building that could benefit from better experimentation or optimization?  Stay tuned for more posts in our series on integrating SigOpt with various ML frameworks to solve **real problems more efficiently!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "experiment = conn.experiments(experiment.id).fetch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_observation = experiment.progress.best_observation\n",
    "print \"Best value: {value}, found at:\\n{assignments}\".format(\n",
    "    value=best_observation.value, \n",
    "    assignments=json.dumps(\n",
    "        best_observation.assignments.to_json(),\n",
    "        sort_keys=True,\n",
    "        indent=4, \n",
    "        separators=(',', ': ')\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sentiment_classifier(positive, negative, assignments):\n",
    "    min_ngram = assignments['min_ngram']\n",
    "    max_ngram = min_ngram + assignments['ngram_offset']\n",
    "    min_doc_frequency = math.exp(assignments['log_min_df'])\n",
    "    max_doc_frequency = min_doc_frequency + assignments['df_offset']\n",
    "    vectorizer = CountVectorizer(\n",
    "        min_df=min_doc_frequency, \n",
    "        max_df=max_doc_frequency,                          \n",
    "        ngram_range=(min_ngram, max_ngram),\n",
    "    )\n",
    "    text_features = vectorizer.fit_transform(positive + negative)\n",
    "    target = [1] * len(positive) + [-1] * len(negative)\n",
    "    \n",
    "    alpha = math.exp(assignments['log_reg_coef'])\n",
    "    l1_ratio = assignments['l1_coef']\n",
    "    classifier = SGDClassifier(\n",
    "        loss='log', \n",
    "        penalty='elasticnet', \n",
    "        alpha=alpha, \n",
    "        l1_ratio=l1_ratio\n",
    "    )\n",
    "    classifier.fit(text_features, target)\n",
    "    return (vectorizer, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "(best_vectorizer, best_classifier) = sentiment_classifier(\n",
    "    POSITIVE_TEXT, \n",
    "    NEGATIVE_TEXT, \n",
    "    best_observation.assignments\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_sentiment(text, verbose=True):\n",
    "    text_features = best_vectorizer.transform([text])\n",
    "    sentiment = best_classifier.predict(text_features)[0]\n",
    "    if verbose:\n",
    "        if sentiment == 1:\n",
    "            print \"positive: {}\".format(text)\n",
    "        else:\n",
    "            print \"negative: {}\".format(text)\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "sentiment = predict_sentiment(random.choice(POSITIVE_TEXT))\n",
    "sentiment = predict_sentiment(random.choice(NEGATIVE_TEXT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View your final results at https://sigopt.com/experiment/2328\n"
     ]
    }
   ],
   "source": [
    "print \"View your final results at https://sigopt.com/experiment/{0}\".format(experiment.id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
