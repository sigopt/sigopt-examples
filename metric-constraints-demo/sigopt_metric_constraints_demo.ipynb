{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMOLmD2zHlpI7/kIoAQsWcP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sigopt/sigopt-examples/blob/main/metric-constraints-demo/sigopt_metric_constraints_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGFIbku-YmSX"
      },
      "source": [
        "This Colab Notebook showcases the [Metric Constraints](https://app.sigopt.com/docs/overview/metric_constraints) feature in SigOpt, as described in [this blog post](https://sigopt.com/blog/metric-constraints-demo/). We use the Metric Constraints feature to optimize for the top-1 accuracy of a CNN with a constraint of the size of the network. We demonstrate this feature using the German Traffic Signs Dataset (GTSRB)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJsP9_YKZRB6"
      },
      "source": [
        "! pip install sigopt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYrf3RnpNzzH"
      },
      "source": [
        "from copy import deepcopy\n",
        "import numpy\n",
        "import pickle\n",
        "from sigopt import Connection\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mITokjXjW7sm"
      },
      "source": [
        "Loading Data. Refer to [this colab notebook](https://colab.research.google.com/drive/17wpJYYMOIzbnaW18Wn7dXT91ahRIJ0N_) on the data augmentation pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXqyxpeLNwqQ"
      },
      "source": [
        "relative_path = \"./drive/My Drive/Colab Notebooks/\"\n",
        "training_file = relative_path + \"traffic-signs-data/train_extended.p\"\n",
        "validation_file= relative_path + \"traffic-signs-data/valid.p\"\n",
        "testing_file = relative_path + \"traffic-signs-data/test.p\"\n",
        "\n",
        "with open(training_file, mode='rb') as f:\n",
        "  train = pickle.load(f)\n",
        "with open(validation_file, mode='rb') as f:\n",
        "  valid = pickle.load(f)\n",
        "with open(testing_file, mode='rb') as f:\n",
        "  test = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IupjljQQPH60"
      },
      "source": [
        "X_train = train['features']\n",
        "y_train = train['labels']\n",
        "X_valid = valid['features']\n",
        "y_valid = valid['labels']\n",
        "X_test = test['features']\n",
        "y_test = test['labels']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJ4nFcdLLKkx"
      },
      "source": [
        "# Sometimes the validation set and testing set images are saved with intensity level of [0, 255]. Convert these to [0, 1].\n",
        "if X_valid.dtype == numpy.uint8:\n",
        "  X_valid = (X_valid / 256).astype('float32')\n",
        "if X_test.dtype == numpy.uint8:\n",
        "  X_test = (X_test / 256).astype('float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIGU_UhnN_NW"
      },
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsxisPtkLPYI"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime â†’ \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVx3ufmKO34t"
      },
      "source": [
        "# Data constant variables\n",
        "NUM_CLASSES = 43\n",
        "IMG_SIZE = 32\n",
        "# Training constant variables\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kihoLQtuPFpe"
      },
      "source": [
        "y_train_cat = keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
        "y_valid_cat = keras.utils.to_categorical(y_valid, NUM_CLASSES)\n",
        "y_test_cat = keras.utils.to_categorical(y_test, NUM_CLASSES)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vU0wKrn9YAD0"
      },
      "source": [
        "Setting up the Keras model, parameterizing the hyperparameters that we want to tune, and evaluating the metrics that we want to track, optimize, or constrain. The CNN model is inspired by the [*MicronNet*](https://arxiv.org/abs/1804.00497) model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7ZBHbf2Tf_0"
      },
      "source": [
        "class TrainingTimeLogger(keras.callbacks.Callback):\n",
        "  def on_train_begin(self, logs={}):\n",
        "    self.time = 0\n",
        "    self.start_time = time.time()\n",
        "\n",
        "  def on_train_end(self, logs={}):\n",
        "    self.time = time.time() - self.start_time\n",
        "\n",
        "def define_munet(hps):\n",
        "  fc_1 = int(hps['fc_1'])\n",
        "  fc_2 = int(hps['fc_2'])\n",
        "  kernel_size_1 = int(hps['kernel_size_1'])\n",
        "  kernel_size_2 = int(hps['kernel_size_2'])\n",
        "  kernel_size_3 = int(hps['kernel_size_3'])\n",
        "  num_filters_1 = int(hps['num_filters_1']) \n",
        "  num_filters_2 = int(hps['num_filters_2'])\n",
        "  num_filters_3 = int(hps['num_filters_3'])\n",
        "\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(layers.Conv2D(3, (1, 1), input_shape=(IMG_SIZE, IMG_SIZE, 3),activation='relu'))\n",
        "  model.add(layers.Conv2D(num_filters_1, (kernel_size_1, kernel_size_1), activation='relu'))\n",
        "  model.add(layers.MaxPooling2D(pool_size=(3, 3), strides=2))\n",
        "\n",
        "  model.add(layers.Conv2D(num_filters_2, (kernel_size_2, kernel_size_2), padding='same', activation='relu'))\n",
        "  model.add(layers.MaxPooling2D(pool_size=(3, 3), strides=2))\n",
        "\n",
        "  model.add(layers.Conv2D(num_filters_3, (kernel_size_3, kernel_size_3), padding='same', activation='relu'))\n",
        "  model.add(layers.MaxPooling2D(pool_size=(3, 3), strides=2))\n",
        "\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(fc_1, activation='relu'))\n",
        "  model.add(layers.Dense(fc_2, activation='relu'))\n",
        "  model.add(layers.Dense(NUM_CLASSES, activation='softmax'))\n",
        "  return model\n",
        "\n",
        "def train_model(model):\n",
        "  time_callback = TrainingTimeLogger()\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    opt = tf.keras.optimizers.SGD(lr=0.01, decay=1e-4, momentum=0.9, nesterov=True)\n",
        "    model.compile(\n",
        "      optimizer=opt,\n",
        "      loss='categorical_crossentropy',\n",
        "      metrics=['categorical_accuracy']\n",
        "    )\n",
        "    datagen = ImageDataGenerator()\n",
        "    history = model.fit_generator(datagen.flow(\n",
        "      X_train, y_train_cat, batch_size=32),\n",
        "      steps_per_epoch=len(y_train) // 32,\n",
        "      epochs=EPOCHS,\n",
        "      validation_data=(X_valid, y_valid_cat),\n",
        "      callbacks=[time_callback],\n",
        "      shuffle=True,\n",
        "      verbose=2,\n",
        "    )\n",
        "    train_time = time_callback.time\n",
        "    validation_accuracy = history.history['val_categorical_accuracy'][-1]\n",
        "    test_accuracy = model.evaluate(X_test, y_test_cat)[1]\n",
        "  return validation_accuracy, test_accuracy, train_time, history\n",
        "\n",
        "\n",
        "MIN_ACCEPTABLE_VAL_ACCURACY = 0.2\n",
        "\n",
        "def create_observation(suggestion):\n",
        "  model = None\n",
        "  size = 0\n",
        "  val_accuracy = 0\n",
        "  test_accuracy = 0\n",
        "  training_time = 0\n",
        "  try:\n",
        "    model = define_munet(suggestion.assignments)\n",
        "    size = model.count_params() / 1e6\n",
        "    val_accuracy, test_accuracy, training_time, history = train_model(model)\n",
        "  except ValueError as e:\n",
        "    print(f'ValueError {e} with {suggestion.assignments.values()}')\n",
        "    return {\n",
        "      'suggestion': suggestion.id,\n",
        "      'failed': True,\n",
        "      'metadata': dict(\n",
        "          error_msg=e,\n",
        "      )\n",
        "    }\n",
        "  # Sometimes the model diverges, going to mark these as failures instead\n",
        "  if val_accuracy <= MIN_ACCEPTABLE_VAL_ACCURACY:\n",
        "    return {\n",
        "      'suggestion': suggestion.id,\n",
        "      'failed': True,\n",
        "      'metadata': dict(\n",
        "        error_msg='divergence',\n",
        "        validation_accuracy=val_accuracy,\n",
        "        training_time=training_time,\n",
        "        loss=repr([\"%.5f\" % l for l in history.history['loss']])\n",
        "      )\n",
        "    }\n",
        "  return {\n",
        "    'suggestion': suggestion.id,\n",
        "    'values': [\n",
        "      {'name': 'size', 'value': size},     \n",
        "      {'name': 'validation_accuracy', 'value': val_accuracy},\n",
        "      {'name': 'test_accuracy', 'value': test_accuracy},\n",
        "      {'name': 'training_time', 'value': training_time},\n",
        "    ],\n",
        "  }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plN9d_OsxGim"
      },
      "source": [
        "Setting up the SigOpt experiment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3tInWOaRpUQ"
      },
      "source": [
        "experiment_meta = dict(\n",
        "  name=\"Traffic Dataset, Constraint Metric v2\",\n",
        "  parameters=[\n",
        "    dict(name=\"kernel_size_1\", bounds=dict(min=2, max=7), type=\"int\"),\n",
        "    dict(name=\"kernel_size_2\", bounds=dict(min=2, max=7), type=\"int\"),\n",
        "    dict(name=\"kernel_size_3\", bounds=dict(min=2, max=7), type=\"int\"),\n",
        "    dict(name=\"num_filters_1\", bounds=dict(min=10, max=50), type=\"int\"),\n",
        "    dict(name=\"num_filters_2\", bounds=dict(min=30, max=70), type=\"int\"),\n",
        "    dict(name=\"num_filters_3\", bounds=dict(min=40, max=160), type=\"int\"),\n",
        "    dict(name=\"fc_1\", bounds=dict(min=10, max=1000), type=\"int\"),\n",
        "    dict(name=\"fc_2\", bounds=dict(min=10, max=1000), type=\"int\"),\n",
        "  ],\n",
        "  metrics = [\n",
        "    dict(\n",
        "      name='size',\n",
        "      objective='minimize',\n",
        "      strategy='constraint',\n",
        "      threshold=0.25\n",
        "    ),\n",
        "    dict(\n",
        "      name='validation_accuracy',\n",
        "      objective='maximize',\n",
        "      strategy='optimize',\n",
        "    ),\n",
        "    dict(\n",
        "      name='test_accuracy',\n",
        "      objective='maximize',\n",
        "      strategy='store',\n",
        "    ),\n",
        "     dict(\n",
        "      name='training_time',\n",
        "      objective='minimize',\n",
        "      strategy='store',\n",
        "    ),\n",
        "  ],\n",
        "  metadata=dict(\n",
        "    training_file=training_file,\n",
        "    validation_file=validation_file,\n",
        "    testing_file=testing_file,\n",
        "    environment='Tesla P100-PCIE',\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    min_acceptable_accuracy=MIN_ACCEPTABLE_VAL_ACCURACY\n",
        "  ),\n",
        "  observation_budget=200,\n",
        "  parallel_bandwidth=1,\n",
        "  project='cm-blog'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yL7ZjKHpaoA"
      },
      "source": [
        "conn = Connection(client_token='YOUR_SIGOPT_API_TOKEN')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9WZVvjbphqM"
      },
      "source": [
        "experiment = conn.experiments().create(**experiment_meta)\n",
        "# experiment.id\n",
        "print(f'Created experiment: https://app.sigopt.com/experiment/{experiment.id}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjHk2jdtpuPW"
      },
      "source": [
        "for i in range(experiment.observation_budget):\n",
        "  s = conn.experiments(experiment.id).suggestions().create()\n",
        "  obs = create_observation(s)\n",
        "  conn.experiments(experiment.id).observations().create(**obs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVftk9fLtiQ6"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3--aB4yGxWSa"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3C-5a27kxXbO"
      },
      "source": [
        "Updating the threshold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1g7r3bfxWcw"
      },
      "source": [
        "experiment = conn.experiments(experiment.id).update(\n",
        "  metrics = [\n",
        "    dict(\n",
        "      name='size',\n",
        "      threshold=0.15\n",
        "    ),\n",
        "    dict(\n",
        "      name='validation_accuracy',\n",
        "    ),\n",
        "    dict(\n",
        "      name='test_accuracy',\n",
        "    ),\n",
        "     dict(\n",
        "      name='training_time',\n",
        "    ),\n",
        "  ],\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVCYesWN0BAU"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}